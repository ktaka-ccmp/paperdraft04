\section{Introduction}

Recently, Linux containers have drawn significant amount of attention because they are lightweight, portable, and repeatable.
Linux containers are generally more lightweight than virtual machine (VM) clusters, 
because the containers share the kernel with the host operating system (OS), even though they maintain separate execution environments. 
They are generally portable because the process execution environments are archived into tar files, 
so whenever one attempts to run a container, the exact same file systems are restored from the archives 
even when totally different data centers are used. 
This means that containers can provide repeatable and portable execution environments.
%
For the same reasons, Linux containers are attractive for web services as well, 
and it is expected that web services consisting of container clusters would be 
capable of being migrated easily for variety of purposes. For example disaster recovery, 
cost performance improvemets, legal compliance, and shortening the geographical distance to customers 
are the main concerns for web service providers in e-commece, gaming, Financial technology(Fintech) and Internet of Things(IoT) field.
%

Kubernetes\cite{K8s2017}, which is one of the popular container cluster management systems, 
enables easy deployment of container clusters.
Since Kubernetes hides the differences in the base environments, users can easily deploy a web service on different 
cloud providers or on on-premise data centers, without adjusting the container cluster configurations to the new environment. 
This allows a user to easily migrate a web service consisting of a container cluster even to the other side of the world. 
A user starts the container cluster in the new location, route the traffic there, 
then stop the old container cluster at his or her convenience.
This is a typical web service migration scenario.

However, this scenario only works when the user migrates a container cluster among major cloud providers including Google Cloud Platform (GCP), 
Amazon Web Services (AWS), and Microsoft Azure.
The Kubernetes does not provide generic ways to route the traffic from the internet into container cluster and is 
heavily dependent on external load balancers that are set up on the fly by cloud providers through their application protocol interfaces (APIs).
%
These external load balancers distribute incoming traffic to every server that hosts containers.
The traffic is then distributed again to destination containers using iptables destination 
network address translation (DNAT)\cite{MartinA.Brown2017,Marmol2015} rules in a round-robin manner. 
The problem happens in the environment with a load balancer that is not supported by the Kubernetes, 
e.g. in an on-premise data center with a bare metal load balancer. 
In such environments, the user needs to manually configure 
the static route for inbound traffic in an ad-hoc manner. 
Since the Kubernetes fails to provide a uniform environment from a container cluster viewpoint,
migrating container clusters among the different environments will always be a burden.

In order to solve this problem by eliminating the dependency on external load balancers,
we have proposed a containerized software load balancer that is run by Kubernetes as  
a part of web services consisting of container cluster in the previous work\cite{takahashi2018portable}.
It enables a user to deploy a web service on different environments without modification easily 
because the web service itself includes load balancers.

We containerized Linux kernel's Internet Protocol Virtual Server (IPVS)\cite{Zhang2000} 
Layer 4 load balancer using an existing Kubernetes ingress\cite{K8sIngress2017} framework, as a proof of concept.
%
%
We also proved that our approach does not significantly deteriorate the performance,  
by comparing the performance of our proposed load balancer with those of 
iptables DNAT load balancer and the Nginx Layer 7 load balancing. 
%
The results indicated that the proposed load balancer could improve the portability of container clusters 
without performance degradation compared with the existing load balancer.

However, in our previous work, we have not discussed the redundancy of such load balancers.
Routing traffic to one of the load balancers while keeping redundancy in the container environment is a complex issue,
because standard Layer 2 rendandacy protocols, e.g. Virtual Router Redundancy Protocol(VRRP)\cite{hinden2004virtual} or OSPF\cite{moy1997ospf} that uses multicast, 
can not be used in many cases.
Further more, providing uniform methods independent of various cloud environments and on-premise datacenter is much more difficult.   

In this paper, we extend the previous work and propose a software load balancer with Equal Cost Multi-Path(ECMP)\cite{thaler2000multipath} redundancy for container cluster environment without a cloud load balancer.
We containerize an open source BGP software, exabgp\cite{exa-networks_2018}, and launch it with containerized Linux kernels IPVS load balancer in a single pod using Kubernetes.
We set up a redundant load balancer cluster using such pods and evaluate its functionality.  
We also measure preliminary performance of such a load balancer cluster.

The contributions of this paper are as follows: 
1) We propose a portable software load balancer with ECMP redundacy for container cluster environments without cloud load balancers.
2) We implement such a load balancer using existing Open Source Software(OSS).
2) We demonstrate the basic functionality, i.e., redundancy and scalability of such s load balancers.
Proving that a user can design and implement a scalable web service using OSS tools by himself will benefit the web service to migrate outside of the major cloud providers.

The rest of the paper is organized as follows.
Section \ref{Related Work} highlights work that deals specifically with container cluster migration, 
software load balancer containerization, and load balancer related tools within the context of the container technology. 
Section \ref{Load balancers in Kubernetes cluster} will explain existing architecture problems and propose our solutions.
In Section \ref{Performance Measurement}, experimental conditions and the parameters 
that we considered to be important in our experiment will be described in detail.
Then, we will show our experimental results and discuss the obtained performance characteristics in Section~\ref{Result and Discussion},  
which is followed by a summary of our work in Section~\ref{Conclusions}.

\section{Related Work}\label{Related Work}

This section highlights related work, especially that dealing with container cluster migration, 
software load balancer containerization, load balancer tools within the context of the container technology and scalable load balancer in the cloud providers.

\paragraph{\bf Container cluster migration:}

Kubernetes developers are trying to add federation\cite{K8sFederation2017} capability for handling situations 
where multiple Kubernetes clusters\footnote{The {\em Kubernetes cluster} refers to a server cluster 
controlled by the Kubernetes container management system, in this paper.} 
are deployed on multiple cloud providers or on-premise data centers, 
and are managed via the Kubernetes federation API server (federation-apiserver). 
However, how each Kubernetes cluster is run on different types of cloud providers
and/or on-premise data centers, especially when the load balancers of such environments are not supported by Kubernetes, 
seems beyond the scope of that project. 
The main scope of this paper is to make Kubernetes usable in environments 
without supported load balancers by providing a containerized software load balancer.

\paragraph{\bf Software load balancer containerization:}
As far as load balancer containerization is concerned, the following related work has been identified:
Nginx-ingress\cite{Pleshakov2016,NginxInc2016} utilizes the ingress\cite{K8sIngress2017} capability of Kubernetes, 
to implement containerized Nginx proxy as a load balancer. Nginx itself is famous as a high-performance web server program
that also has the functionality of a Layer-7 load balancer. Nginx is capable of handling Transport Layer Security(TLS) encryption, 
as well as Uniform Resource Identifier(URI) based switching. However, the flip side of Nginx is that it is much slower than Layer-4 switching.
We compared the performance between Nginx as a load balancer and our proposed load balancer in this paper.
%
Meanwhile, the kube-keepalived-vip\cite{Prashanth2016} project is trying to use Linux kernel's IPVS\cite{Zhang2000} 
load balancer capabilities by containerizing the keepalived\cite{ACassen2016}.
The kernel IPVS function is set up in the host OS's net name spaces and is shared among multiple web services,
as if it is part of the Kubernetes cluster infrastructure.
Our approach differs in that the IPVS rules are set up in container's net name spaces 
and function as a part of the web service container cluster itself.
The load balancers are configurable one by one, and are  movable with the cluster once the migration is needed.
The kube-keepalived-vip's approach lacks flexibility and portability whereas ours provide them.
%
The swarm mode of the Docker\cite{DockerCoreEngineering2016,DockerInc2017} also uses IPVS for internal load balancing,
but it is also considered as part of Docker swarm infrastructure, 
and thus lacks the portability that our proposal aims to provide.

\paragraph{\bf Load balancer tools in the container context:}
There are several other projects where efforts have been made to utilize IPVS in the context of container environment.
For example, GORB\cite{Sibiryov2015} and clusterf\cite{Aaltodoc:http://urn.fi/URN:NBN:fi:aalto-201611025433} are daemons 
that setup IPVS rules in the kernel inside the Docker container. 
They utilize running container information stored in key-value storages
like Core OS etcd\cite{CoreOSEtcd} and HashiCorp's Consul\cite{HashiCorpConsul}. 
Although these were usable to implement a containerized load balancer in our proposal, we did not use them, 
since Kubernetes ingress framework already provided the methods to retrieve running container information through standard API.

\paragraph{\bf Cloud load balancers:}

As far as the cloud load balancers are concerned, two articles have been identified.
Google's maglev\cite{eisenbud2016maglev} is a software load balancer used in Google Cloud Platform(GCP)\cite{Voellm2013}.
Maglev uses modern technologies including per flow ECMP and kernel bypass for userspace packet processing.
Maglev serves as the GCP's load balancer that is used by the Kubernetes.
Maglev can be solely used in GCP, and the users need open source software load balancer that is runnable even in on-premise data centers.
Microsoft's Ananta\cite{patel2013ananta} is another software load balancer implementation using ECMP and windows network stack.
Ananta can be solely used in Microsoft's Azure cloud infrastructure\cite{patel2013ananta}.
The proposed load balancer by the author is different in that it is aimed to be used in every cloud provider and on-premise data centers.

\section{Proposed Architecture}\label{Architecture}

Here we discuss the overall architecture of the proposed load balancers.

\subsection{A containerized Load balancer}

\subsubsection{Problems of Kubernetes Cluster}

\begin{figure}
\includegraphics[width=\columnwidth]{Figs/K8sConventional}
\caption{Conventional architecture of a Kubernetes cluster.}
\label{fig:K8sConventional}
\end{figure}

Problems commonly occur when the Kubernetes container management system is used outside of recommended cloud providers(such as GCP or AWS).
Figure~\ref{fig:K8sConventional} shows an exemplified Kubernetes cluster.
A Kubernetes cluster typically consists of a master and nodes. They can be physical servers or VMs.
On the master, daemons that control the Kubernetes cluster are typically deployed. 
These daemons include, apiserver, scheduler, controller-manager and etcd. 
On the nodes, the kubelet daemon will run {\it pods}, depending the PodSpec information obtained from the apiserver on the master.
A {\em pod} is a group of containers that share same net name space and cgroups, 
and is the basic execution unit in a Kubernetes cluster.

When a service is created, the master will schedule where to run {\em pods} and kubelets on the nodes will launch them accordingly.
At the same time, the masters will send out requests to cloud provider API endpoints, asking them to set up external load balancers.
The proxy daemon on the nodes will also setup iptables DNAT\cite{MartinA.Brown2017} rules. 
The Internet traffic will then be evenly distributed by the external load balancer to nodes, 
after which it will be distributed again by the DNAT rules on the nodes to the designated {\em pods}. 
The returning packets will follow the exact same route as the incoming ones.

This architecture has the followings problems: 
1) Having external load balancers whose APIs are supported by the Kubernetes daemons is a prerequisite. 
There are numerous load balancers which is not supported by the Kubernetes.
These include the bare metal load balancers for on-premise data centers.  
In such cases, a user could manually setup the routing table on the gateway so that the traffic would be routed to one of the nodes.
Then the traffic would be distributed by the DNAT rules on the node to the designated {\em pods}.
However, this approach would require complicated network configuration and significantly degrade the portability of container clusters.
2) Distributing the traffic twice, first on the external load balancers and second on each node, 
complicates the administration of packet routing. 
Imagine a situation in which the DNAT table on one of the nodes malfunctions.
In such a case, only occasional timeouts would be observed, which would make it very difficult to find out which node was malfunctioning.   

In short, 1) Kubernetes can be used only in limited environments where the external load balancers are supported, 
and 2) the routes incoming traffic follow are very complex.

\subsubsection{Proposed architecture with a potable load balancer}

In order to address these problems, we propose a containerized software load balancer 
that is deployable in any environment even if there are no external load balancers.

\begin{figure}
\includegraphics[width=\columnwidth]{Figs/K8sProposed}
\caption{Kubernetes cluster with proposed load balancer.}
\label{fig:K8sProposed}
\end{figure}

Figure~\ref{fig:K8sProposed} shows the proposed  Kubernetes cluster architecture, 
which has the following characteristics:
1) Each load balancer itself is run as a {\em pod} by Kubernetes. 
2) Load balancer configurations are dynamically updated based on information about running {\em pods}.
%%3) There exist multiple load balancers for redundancy. 
The proposed load balancer can resolve the conventional architecture problems, as follows:
Since the load balancer itself is containerized, load balancer can run in any environment including on-premise data centers, 
even without external load balancers that is supported by Kubernetes.
The incoming traffic is directly distributed to designated {\em pods} by the load balancer. 
It makes the administration, e.g. finding malfunctions, easier.

We designed the proposed load balancer using three components, IPVS, keepalived, and a controller. 
These components are placed in a Docker container image.
The IPVS is a Layer-4 load balancer capability, which is included in the Linux kernel 2.6.0 released in 2003 or later, 
to distribute incoming Transmission Control Protocol(TCP) traffic to 
{\em real servers}\footnote{The term, {\em real servers} refers to worker servers that will respond to incoming traffic, 
in the original literature\cite{Zhang2000}. We will also use this term in the similar way.}\cite{Zhang2000}. 
For example, IPVS distributes incoming Hypertext Transfer Protocol(HTTP) traffic destined for a single destination IP address, 
to multiple HTTP servers(e.g. Apache HTTP or nginx) running on multiple nodes in order to improve the performance of web services.
Keepalived is a management program that performs health checking for {\em real servers}
and manage IPVS balancing rules in the kernel accordingly.
It is often used together with IPVS to facilitate ease of use.
The controller is a daemon that periodically monitors the {\em pod} information on the master, 
and performs various actions when such information changes.
Kubernetes provides ingress controller framework as the Go Language(Golang) package to implement such controllers. 
We have implemented a controller program that will feed {\em pod} state changes to keepalived 
using this framework. 


\subsection{Load balancer redundancy}

\subsubsection{Overlay network}

In order to discuss redundancy, the knowledge of the overlay network is essential.
We briefly explain an abstract concept of overlay network that is common to existing overlay network including flannel\cite{coreos_2018} and calico\cite{project_calico}.

\begin{figure}[tb]
\begin{center}
\includegraphics[width=\columnwidth]{Figs/overlay.png}
\end{center}
\caption{
  The network architecture of an exemplified container cluster system.
  A load balancer(lb) pod(the white box with "lb") and web pods are running on nodes(the blue boxes).
  The traffic from the internet are forwarded to the lb pod by the upstream router using the node network,
  and the distributed to web pods using the overlay network.
}
\label{fig:overlay}
\end{figure}

Fig.~\ref{fig:overlay} shows schematic diagram of network architecture of a container cluster system. 
An overlay network consists of appropriate routing tables on nodes, and optionally of tunneling setup using ipip or vxlan.
The overlay network is the network for containers to communicate with each other. 
The node network is the network for nodes to communicate with each other. 
The upstream router usually belongs to the node network.
When the ipvs container in the Fig.~\ref{fig:overlay} communicates with the other nodes, 
the nodes can properly route the packets because they have the routes to 172.16.0.0/16 in their routing table, 
which is a part of overlay network setups.
When a container communicates with the upstream router that has no knowledge of the overlay network, the souce IP address must be translated by Source Network Address Translation(SNAT) rules on each node.

\subsubsection{Redundancy with ECMP}

The ECMP is a functionality a router may support, where the router has multiple next hops with equal cost(priority) to a destination, and generally distribute the traffic depending on the hash of the flow 5 tuples(source IP, destination IP, source port, destination port, protocol).
The multiple next hops and their cost are often populated using the Border Gateway Protocol(BGP) protocol.

\begin{figure}[tb]
\begin{center}
\includegraphics[width=\columnwidth]{Figs/ecmp.png}
\end{center}
\caption{
  The proposed architecture of load balancer redundancy.
  The traffic from the internet is distributed by the upstream router to multiple of lb pods using hash-based ECMP and then distributed by the lb pods to web pods using Linux kernel's ipvs.
  The ECMP routing table on the upstream router is populated using iBGP.
}
\label{fig:ecmp}
\end{figure}

Fig.~\ref{fig:ecmp} shows our proposed redundancy architecture with ECMP for software load balancer containers.
%
We propose to use a node with the knowledge of overlay network as a Route Reflector, in other to alleviate the following problems.
1) If we were to setup BGP peering directly between a load balancer container and the router, the IP address of the container should be translated using SNAT rules by a node, since the router has no knowledge of the overlay network and cannot send out returning packet correctly.
However, if we use SNAT, the router only see the node IP address as the peer, and hence the router cannot set up multiple BGP session from a single node.
This problem poses limitation that a single node can accommodate only one load balancer at most.

2) Also, if we were to setup BGP peering directly between a load balancer container and the router, the router should support so-called dynamic peering(or dynamic neighbor) functions, where one does not have to write exact IP address of peers in the configuration.
This function is essential because one can not predict exact IP addresses of the load balancer containers before he launches them.
Even if the router supports such functions, the administrator of the upstream router may not like accepting dynamic peering from unknown random IP addresses for security reasons.

By introducing route reflectors we can configure them as we like, e.g., we can make them so that BGP sessions with load balancers are set up dynamically, and multiple BGP sessions from a single node can be established.
The upstream router does not need to accept BGP session from containers with random IP addresses, but only from the Router Reflecotr with well known fixed IP address.
Although not shown in the Fig.~\ref{fig:ecmp}, we could also place another Route Reflector for redundancy.

The notable benefit of the ECMP setup is the fact that it is scalable.
All load balancer that claims as the next hop is active, i.e., the traffic is forwarded to them depending on the flow tuple.
The traffic is distributed by the upstream router. Hence the overall throughput is determined by the router.
If a software load balancer is capable of handling 1 Gbps equivalent of traffic and the upstream router is capable of handling 10 Gbps, it is worthwhile launching 10 of the software load balancer containers to fill up maximum throughput of the upstream router.

\subsubsection{Redundancy with VRRP}

\begin{figure}[tb]
\begin{center}
\includegraphics[width=\columnwidth]{Figs/vrrp.png}
\end{center}
\caption{
  An alternative redundant load balancer architecture using.
  The traffic from the internet is forwarded by the upstream router to a active lb node and then distributed by the lb pods to web pods using Linux kernel's ipvs.
  The active lb pod is selected using VRRP protocol.
}
\label{fig:vrrp}
\end{figure}

Fig.~\ref{fig:vrrp} shows an alternative redundancy setup using the VRRP protocol.
In the case of VRRP, the load balancer container should be able to use the node net namespace for the following two reasons.
1) When failover occurs, the new master sends gratuitous Adress Resolution Packets(ARP) packets to update the ARP cache of the upstream router during the transition.
Such gratuitous ARP packets should consist of the virtual IP address shared by the load balancers and the MAC address of the node where the new master load balancer is running.
Programs that send out gratuitous ARP with node MAC address should be in the node net namespace.
%
2) Furthermore, the active load balancer sends out periodic advertisement using UDP multicast packet to inform existence of itself.
The receiving load balancer in backup state stays calm unless the VRRP advertisement stops for a specified duration of time.
The UDP multicast is often unsupported in overlay network used by container cluster environment, and hence the load balancer needs to be able to use the node net namespace.

VRRP programs also support unicast advertisement by specifying IP addresses of peer load balancers when it starts.
However, container cluster management system randomly assign IP addresses of containers when it launches them.
Therefore the unicast mode is not feasible in container cluster environment.

The other drawback compared with the ECMP case is that the redundancy of VRRP is provided in Active-Backup manner.
This means that a single software load balancer limits the overall performance of the entire container cluster.

\section{Implementation}\label{Implementation}

Here we discuss the implementation of our proposed load balancers with ECMP redundancy.

\subsection{Proof of concept system architecture}

\begin{figure}[tb]
\begin{center}
\includegraphics[width=\columnwidth]{Figs/poc.png}
\end{center}
\caption{
  A proof of concept container cluster with porposed redundant software balancers.
  The master and nodes are configured as Kubernetes's master and nodes on top of conventional linux boxes, respectively.
  The route reflector and the upstream router are also conventional linux boxes.
}
\label{fig:poc}
\end{figure}

Fig.~\ref{fig:poc} shows the shcematic diagram of proof of concept container cluster system with our porposed redundant software balancers.
All the nodes and route reflector are configured using Debian 9.5 with self compiled linux-4.16.8 kernel.  
The upstream router also used conventional linux box using the same OS as the nodes and route reflector.
For the linux kernel to support hash based ECMP routing table we needed to use kernel version 4.12 or later.

The load balancer pods consist of an exabgp container and an ipvs container.
The ipvs container is responsible for distributing the traffic to a service IP to web(nginx) server pods.
The ipvs pods monitor the availability of web server pods and manage the load balancing rule appropriately.
The exabgp container is responsible for advertising the route to a service IP, to the route reflector
The route reflector aggregates the routing information advertised by load balancer pods and advertise them to the upstream router.

\subsection{IPVS container}

\begin{figure}
\includegraphics[width=\columnwidth]{Figs/ipvs-ingress-schem}
\caption{Implementation of IPVS container.}
\label{fig:ipvs-ingress-schem}
\end{figure}

The proposed load balancer needs to dynamically reconfigure the IPVS balancing rules whenever {\em pods} are created/deleted. 
Figure~\ref{fig:ipvs-ingress-schem} is a schematic diagram of IPVS container to show the dynamic reconfiguration of the IPVS rules.
Two daemon programs, controller and keepalived, run in the container inside the LB2 pod are illustrated.
The keepalived manages Linux kernel's IPVS rules depending on the ipvs.conf configuration file.
It is also capable of health-checking the liveliness of {\em real server}, 
which is represented as a combination of the IP addresses and port numbers of the target {\em pods}. 
If the health check to a {\em real server} fails, keepalived will remove that {\em real server} from the IPVS rules.

The controller monitors information concerning the running {\em pods} of a service 
in the Kubernetes cluster by consulting the apiserver running on the master.
Whenever {\em pods} are created or deleted, the controller will automatically regenerate an appropriate ipvs.conf 
and issue SIGHUP to keepalived.
Then, keepalived will reload the ipvs.conf and modify the kernel's IPVS rules accordingly.
The actual controller\cite{ktaka_ccmp_2017_826894} is implemented using the Kubernetes ingress controller\cite{K8sIngress2017} framework. 
By importing existing Golang package, \enquote{k8s.io/ingress /core/pkg/ingress}, we could simplify the implementation, e.g. 
120 lines of code.  

%In this way, the IPVS's balancing rules inside Linux kernel are maintained so that it can distribute the incoming traffic only to the living pods.

Configurations for capabilities were needed in the implementation: adding the CAP\_SYS\_MODULE capability 
to the container to allow the kernel to load required kernel modules inside a container, 
and adding CAP\_NET\_ADMIN capability to the container to allow keepalived to manipulate the kernel's IPVS rules. 
For the former case, we also needed to mount the \enquote{/lib/module} of the node's file system on the container's file system.

\subsection{BGP software container}

\begin{figure}
\includegraphics[width=\columnwidth]{Figs/exabgp}
\caption{Implementation of exabgp container.}
\label{fig:exabgp}
\end{figure}

\begin{table}[tb]% 
\begin{center}
\begin{tabular}{l}
\hline
\textbf{BGP announcement:}\\
\hspace*{5mm} route 10.1.1.0/24 next-hop 10.0.0.106 \\
\textbf{Routing in node net name space:}\\
\hspace*{5mm} ip netns exec node ip route replace 10.1.1.0/24 dev docker0 \\
\textbf{Accept as local:}\\
\hspace*{5mm} ip route add local 10.1.1.0/24 dev eth0 \\
\hline
\end{tabular}
\halflineskip

\caption{
  The required settings in exabgp container.
  The node IP address, 10.0.0.106 is used as next-hop for the service IP range 10.1.1.0/24 in BGP annoucement.
  In oder to route the packets toward the service IPs to a container, a routing rule to the dev docker0 is created in the node net namespace.
  A routing rule to accept the packets as local is also required. 
}
\label{table:exabgp}

\end{center}
\end{table}




\section{Evaluation}\label{Evaluation}

\subsection{Redundancy}\label{Redundancy}

\subsection{Scalability}\label{Scalability}


\section{Conclusions}\label{Conclusions}

In this paper, we proposed a portable software load balancer that has the following features, 1) runnable as a Linux container, 2) redundancy with ECMP technique,  for the container cluster systems.

Our load balancer aims at facilitating migration of container clusters for web services.
We implemented a containerized software load balancer that is run by Kubernetes as a part of container cluster,
using Linux kernel's IPVS.

To discuss the feasibility of the proposed load balancer, we built
a Kubernetes cluster system and conducted performance measurements.
Our experimental results indicate that the IPVS based load balancer in container improves the portability of
the container cluster while it shows the similar performance levels as the existing iptables DNAT based load balancer.

We also started to the implementation of a novel software load balancer using recently introduced Linux kernel's XDP infrastructure.
While it is in a preliminary stage of the development, essential functions and design issues have been already clarified.
They will be presented at the time of the presentation.

\section{Future work}\label{Future work}

We leave the following for the future work:
1) Performance measurement of XDP load balancer
2) Implementation of consistent hashing balancing algorithm
3) Containerization of XDP load balancer
These works are ongoing and will be presented when they are available.

BGP peering with the Route Reflector is one way of providing a uniform environment for web service container clusters.
While providing a software load balancer that can be controlled through API is another way.
For the former, corporation by the cloud provider so that users can use BGP peering with the upstream router is essential.
In case such a corporation cannot be made, the latter is also worth investigating.


